# -*- coding: utf-8 -*-
"""batch18_data_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vhP49Z4zxxX--5_YGef1nOcSfqZbs7pH
"""

#handling a csv file
#open the file
#load the data , read the data and then print it
import csv
fp="sample_data.csv"
with open (fp,"r") as f:
    reader=csv.reader(f)
    for i in reader:
        print(i)

data=[['106','shakul malik','40','delhi','50000']] #data to be written in csv file
with open('sample_data.csv','a')as file:
    writer=csv.writer(file)    # creating a CSV writer object
    writer.writerows(data)     # # Writing the list of rows (data) to the CSV file
print("csv has been written successfully")

fp="sample_data.csv"
with open (fp,"r") as f:   # Opening the CSV file in read mode ('r')
    reader=csv.reader(f)   # Creating a CSV reader object to read the file
    for i in reader:       # Iterating through each row in the CSV file
        print(i)

pip install pandas

import pandas as pd
df= pd.read_csv("sample_data.csv")
df

df= pd.read_csv("sentiment_analysis_results.csv")
df

pip install PyPDF2



import PyPDF2
with open("iph16prospecs.pdf","rb") as pdf_file:  # Opening the PDF file in binary read mode ('rb')
    reader=PyPDF2.PdfReader(pdf_file)    # Creating a PdfReader object to read the PDF file

    meta= reader.metadata    # Extracting metadata from the PDF (e.g., title, author, producer, etc.)
    print("Metadata", meta)

    for page,i in enumerate(reader.pages):   # Iterating through each page in the PDF
        text= i.extract_text()   # Extracting text content from the current page
        print("page",page+1,"content : \n ", text)   # Printing the page number and its content

import PyPDF2
import re
with open("iph16prospecs.pdf","rb") as pdf_file:
    reader=PyPDF2.PdfReader(pdf_file)

    headings=set()   # initializing an empty set to store unique headings
    for page in reader.pages:
        text=page.extract_text()    # Extracting text content from the current page
        if text:           # Checking if the page contains any text
            matches= re.findall(r'\b[A-Z]+\b',text)   # Finding all matches of words in uppercase using a regular expression
            headings.update(matches)  # Adding the matches to the set to ensure uniqueness


for i in headings:
    print(i)

import requests

# GitHub API base URL
GITHUB_API_URL = "https://api.github.com"

def get_user_repos(username):
    url=f"{GITHUB_API_URL}/users/{username}/repos"  # Constructing the URL for the user's repositories
    response=requests.get(url)     # Sending a GET request to the GitHub API

    #print(response.content)
    if response.status_code==200:   # Checking if the request was successful
        repos=response.json()     # Parsing the JSON response
        for i in repos:       # Iterating through the list of repositories
            print(f" Repo: {i['name']}, stars:{i['stargazers_count']},URL: { i['html_url']}")
            print(" Repo:", i['name'] , "stars:",i['stargazers_count'],"URL:", i['html_url'])
    else:
        print(f" error:{response.status_code}, message:{response.json()}")

username = "shakul-malik"
get_user_repos(username)

import requests

# API Endpoint for India's GDP Data (Indicator: NY.GDP.MKTP.CD)
url = "http://api.worldbank.org/v2/country/IN/indicator/NY.GDP.MKTP.CD?format=json&per_page=20"

# Make the request
response = requests.get(url)


if response.status_code == 200:
    data = response.json()
   # print(data)
    print("\nIndia's GDP Data (Past 20 Years)\n---------------------------------")
    for entry in data[1]:
        if entry["value"]:
            print(f"Year: {entry['date']}, GDP: ${entry['value']:,.2f}")

else:
    print(f"Error {response.status_code}: {response.text}")

import requests

url = "https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd"
response=requests.get(url)
print(response.content)

import requests

# Step 1: Set up the API key (replace with your actual News API key)
api_key = 'd6b95b5ddeba46ecaba667ddf118cd22'

# Step 2: Define the endpoint and parameters
url = 'https://newsapi.org/v2/everything'
params = {
    'q': 'iphone16',  # Search query for articles about "social media"
    'apiKey': api_key,
    'language': 'en',  # Only return articles in English
    'pageSize': 10  # Limit to 10 results
}

# Step 3: Make the request to the News API
response = requests.get(url, params=params)
#print(response.json())
# Step 4: Parse the response JSON
data = response.json()

# Step 5: Display the articles
if 'articles' in data:
    for article in data['articles']:
        print(f"Title: {article['title']}")
        print(f"Source: {article['source']['name']}")
        print(f"Published At: {article['publishedAt']}")
        print(f"URL: {article['url']}")
        print("-" * 50)
else:
    print("No articles found.")

"""### Web scraping is the process of automatically extracting data from websites. It involves accessing a webpage, parsing its HTML content, and retrieving specific information such as text, images, or structured data.

#### Why Do We Perform Web Scraping?
 #### Data Collection: To gather structured data from websites for analysis and decision-making.
 #### Market Research: To monitor competitors, track trends, and analyze customer behavior.
 #### Price Monitoring: To track product prices and availability in e-commerce.
 #### Content Aggregation: To compile data for news, reviews, or comparison platforms.
 #### Automation: To save time by automating repetitive data-gathering tasks.
 #### Key Point: Web scraping is used to efficiently extract and utilize web data when APIs are unavailable or insufficient.
"""

#Web scraping

import requests
from bs4 import BeautifulSoup

# Step 1: Define the URL to scrape (example: GSMArena search page for iPhone 16)
url = 'https://www.gsmarena.com/results.php3?sQuickSearch=yes&sName=iphone+16'

# Step 2: Make an HTTP request to the website
response = requests.get(url)
#print(response.content)

# # Step 3: Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')
# print(soup)
# # Step 4: Find relevant information (e.g., news articles, reviews, etc.)
# # On GSMArena, articles are usually under a class like 'article-info'
articles = soup.find_all('div', class_='makers')
# print(articles)

# # Step 5: Loop through the articles and print the titles and links
for i in articles:
    phones = i.find_all('li')
    for p in phones:
        phone_name = p.find('span').text
        phone_link = p.find('a')['href']
        print(f"Phone Name: {phone_name}")
        print(f"Link: https://www.gsmarena.com/{phone_link}")
        print('-' * 50)

"""![Screenshot 2025-01-24 093356.png](attachment:0ddbd1c2-cd70-4dd2-bca4-59e90d231a17.png)

![Screenshot 2025-01-24 093440.png](attachment:181a6ab6-ba82-4278-b3dd-e8d1c51fb730.png)
"""

import requests    # Step 1: Importing requests to make HTTP requests
from bs4 import BeautifulSoup    # Step 2: Importing BeautifulSoup for parsing HTML
import pandas as pd


# Step 3: Specify the URL of the website to scrape
url = 'http://books.toscrape.com/'

# Step 4: Send a GET request to fetch the website content
response = requests.get(url)
print(response)
#print(response.content)

if response.status_code == 200:   # Step 5: Check if the request was successful (status code 200)

    soup = BeautifulSoup(response.content, 'html.parser')   # Step 6: Parse the HTML content using BeautifulSoup
    books = soup.find_all('article', class_='product_pod')     # Step 7: Locate all book articles on the page
    titles = []   # Step 8: Initialize empty lists to store book details
    prices = []
    availability = []

    for i in books:            # Step 9: Iterate through all books to extract details
        title = i.h3.a['title']    # Step 10: Extract the book title and append to the titles list
        titles.append(title)


        price = i.find('p', class_='price_color').text    # Step 11: Extract the book price and append to the prices list
        prices.append(price)


        avail = i.find('p', class_='instock availability').text.strip()  # Step 12: Extract availability status and append to the availability list
        availability.append(avail)


    df = pd.DataFrame({           # Step 13: Create a pandas DataFrame to organize the extracted data
        'Title': titles,
        'Price': prices,
        'Availability': availability
    })


    print(df.head(20))
else:
    print('Failed to retrieve data:', response.status_code)

import requests
from bs4 import BeautifulSoup
import pandas as pd

# Amazon iPhone Search URL
url = 'https://www.amazon.in/s?k=iphone+16'

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# Send request
response = requests.get(url, headers=headers)
print(response)
soup = BeautifulSoup(response.content, 'html.parser')
#print(soup)

# # Extracting product details
products = soup.find_all('div', {'data-component-type': 's-search-result'})
#print(products)
data = []
for i in products:
    title = i.h2.text.strip()
    price = i.find('span', 'a-price-whole')
    rating = i.find('span', 'a-icon-alt')

    price = price.text.strip() if price else 'N/A'
    rating = rating.text.strip() if rating else 'N/A'

    data.append({
        'Product': title,
        'Price': price,
        'Rating': rating
    })

# Convert to DataFrame
df = pd.DataFrame(data)
print(df)





!pip install selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

def get_movie_titles_selenium():
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # Run in headless mode
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    driver.get("https://www.rottentomatoes.com/browse/movies_in_theaters")

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # Adjust selector based on actual page structure
    titles = [title.get_text(strip=True) for title in soup.find_all("span", class_="p--small")]

    driver.quit()
    return titles

movie_titles = get_movie_titles_selenium()
print(movie_titles)

import requests

from bs4 import BeautifulSoup

import pandas as pd

url='https://www.rottentomatoes.com/browse/movies_in_theaters/'

response=requests.get(url)

print(response)

response.content[100]

soup=BeautifulSoup(response.content,'html.parser')

data=response.json

data

print(response.text)

def fetch_movies_from_rotten_tomatoes():

    print(" Fetching Latest Movies from Rotten Tomatoes...")

    url = "https://www.rottentomatoes.com/browse/movies_in_theaters"
    headers = {
        "User-Agent": "Mozilla/5.0"  # Mimic a real browser
    }

    try:

        response = requests.get(url, headers=headers)

        #response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")

        #print(soup)
        titles = []

        for i in soup.select('a.js-tile-link'):

            title = i.get("href").split("/")[-1].replace("-", " ").title()

            titles.append(title)

        print(f" Fetched {len(titles)} movies from Rotten Tomatoes!\n")

        return titles[:30]  # Limit to 30 latest movies



    except requests.exceptions.RequestException as e:

        print(f" Error while scraping Rotten Tomatoes: {e}")

        return []
# Test the function

titles = fetch_movies_from_rotten_tomatoes()

print(titles)

[print (title) for title in titles]

#!pip install emoji

import emoji

print("ðŸ¤–")

def fetch_movies_from_rotten_tomatoes():

    print(" Fetching Latest Movies from Rotten Tomatoes...")

    url = "https://www.rottentomatoes.com/browse/movies_in_theaters"
    headers = {
        "User-Agent": "Mozilla/5.0"  # Mimic a real browser
    }

    try:

        response = requests.get(url, headers=headers)

        #response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")

        #print(soup)
        titles = []

        for i in soup.select('a.js-tile-link'):

            title = i.get("href").split("/")[-1].replace("-", " ").title()

            titles.append(title)

        print(f" Fetched {len(titles)} movies from Rotten Tomatoes!\n")

        return titles[:30]  # Limit to 30 latest movies



    except requests.exceptions.RequestException as e:

        print(f" Error while scraping Rotten Tomatoes: {e}")

        return []











# Test the function

titles = fetch_movies_from_rotten_tomatoes()

print(titles)

## WE NOW NEED TO CREATE



import requests
from bs4 import BeautifulSoup

def extract_movies(titles):
    print("Movies are Fetching...")

    movies=[]

    for i in titles:

       # breakpoint()

        url = f"http://www.omdbapi.com/?t={i}&apikey=14ff6ef5"

        try:

            response = requests.get(url)

#             response.raise_for_status()

            data = response.json()

          #  %Debug

            if data.get("Response") == "True":

                movies.append(data)

            else:

                print(f" Movie not found: {i}")

        except requests.exceptions.RequestException as e:

            print(f" Request Error for {i}: {e}")

            print(f" Fetched {len(movies)} movies successfully!\n")

        return movies

exmovies=extract_movies(titles)

print(exmovies)